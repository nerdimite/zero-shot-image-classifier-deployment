{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6899d17",
   "metadata": {},
   "source": [
    "# Zero Shot Image Classification using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95ad0d1-20c0-46a5-90d5-1c138f914c7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:55:26.226668Z",
     "iopub.status.busy": "2021-12-02T20:55:26.226302Z",
     "iopub.status.idle": "2021-12-02T20:55:34.728990Z",
     "shell.execute_reply": "2021-12-02T20:55:34.727598Z",
     "shell.execute_reply.started": "2021-12-02T20:55:26.226599Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-c9xrf7en\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-c9xrf7en\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from clip==1.0) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from clip==1.0) (4.59.0)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from clip==1.0) (1.9.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from clip==1.0) (0.10.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing_extensions in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from torch->clip==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.20.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from torchvision->clip==1.0) (8.2.0)\n",
      "Building wheels for collected packages: clip, ftfy\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369094 sha256=59b0f981b2c99028935eb125929e43950cd1bd3bc9e776daf46589f3f37e64aa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-utuq989v/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41913 sha256=dcd7cacfb4bb4660fe347aef780ade7e057a40a68711f553ed0b8dfae98176ea\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/7f/40/63/4bf603cec3ecc4a26985405834cb47eb8368bfa59e15dde046\n",
      "Successfully built clip ftfy\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adequate-lunch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:56:32.045239Z",
     "iopub.status.busy": "2021-12-02T20:56:32.044908Z",
     "iopub.status.idle": "2021-12-02T20:56:51.780493Z",
     "shell.execute_reply": "2021-12-02T20:56:51.779844Z",
     "shell.execute_reply.started": "2021-12-02T20:56:32.045210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbec0f5",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2c5d0",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "On linux, you can use wget in the terminal to download the pretrained model,\n",
    "```bash\n",
    "wget https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b5d84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:56:54.735959Z",
     "iopub.status.busy": "2021-12-02T20:56:54.735639Z",
     "iopub.status.idle": "2021-12-02T20:57:00.053108Z",
     "shell.execute_reply": "2021-12-02T20:57:00.052435Z",
     "shell.execute_reply.started": "2021-12-02T20:56:54.735935Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = 'ViT-B-32.pt'\n",
    "model, transformations = clip.load(model_path, device='cpu', jit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4b951",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d8f3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:02.024316Z",
     "iopub.status.busy": "2021-12-02T20:57:02.023995Z",
     "iopub.status.idle": "2021-12-02T20:57:02.028093Z",
     "shell.execute_reply": "2021-12-02T20:57:02.027380Z",
     "shell.execute_reply.started": "2021-12-02T20:57:02.024293Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(image, classes):\n",
    "    \n",
    "    image = transformations(image).unsqueeze(0)\n",
    "    classes = clip.tokenize(classes)\n",
    "\n",
    "    return image, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac7f28",
   "metadata": {},
   "source": [
    "### Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38ebba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:03.210922Z",
     "iopub.status.busy": "2021-12-02T20:57:03.210465Z",
     "iopub.status.idle": "2021-12-02T20:57:03.233069Z",
     "shell.execute_reply": "2021-12-02T20:57:03.232351Z",
     "shell.execute_reply.started": "2021-12-02T20:57:03.210895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: dancing.jpg\n",
      "1: elon_masked.jpg\n",
      "2: kids_playing.jpg\n",
      "3: plane.jpg\n",
      "4: traffic.jpg\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "images = []\n",
    "for i, file in enumerate(os.listdir('images')):\n",
    "    img_path = os.path.join('images', file)\n",
    "    image_paths.append(img_path)\n",
    "    images.append(Image.open(img_path))\n",
    "    print(f'{i}: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9173d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:03.592007Z",
     "iopub.status.busy": "2021-12-02T20:57:03.591569Z",
     "iopub.status.idle": "2021-12-02T20:57:03.595172Z",
     "shell.execute_reply": "2021-12-02T20:57:03.594511Z",
     "shell.execute_reply.started": "2021-12-02T20:57:03.591983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"kids playing\", \n",
    "    \"dancing\",\n",
    "    \"elon musk wearing a face mask\", \n",
    "    \"aeroplane\",\n",
    "    \"traffic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22819658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:04.486692Z",
     "iopub.status.busy": "2021-12-02T20:57:04.486044Z",
     "iopub.status.idle": "2021-12-02T20:57:04.492667Z",
     "shell.execute_reply": "2021-12-02T20:57:04.492078Z",
     "shell.execute_reply.started": "2021-12-02T20:57:04.486654Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(image, classes):\n",
    "\n",
    "    # Preprocess inputs\n",
    "    image_input, classes_input = preprocess(image, classes)\n",
    "\n",
    "    # Forward pass on the model\n",
    "    logits_per_image, logits_per_text = model(image_input, classes_input)\n",
    "\n",
    "    # Normalize the cosine distances using softmax\n",
    "    probs = logits_per_image.softmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Format and sort the final output\n",
    "    output = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        output.append(\n",
    "            (classes[i], round(prob, 4))\n",
    "        )\n",
    "\n",
    "    sorted_outputs = sorted(output, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9533562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:14.920693Z",
     "iopub.status.busy": "2021-12-02T20:57:14.920186Z",
     "iopub.status.idle": "2021-12-02T20:57:17.557557Z",
     "shell.execute_reply": "2021-12-02T20:57:17.556793Z",
     "shell.execute_reply.started": "2021-12-02T20:57:14.920666Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: images/dancing.jpg\n",
      "Output:\n",
      "[('A photo of dancing', 0.8808), ('A photo of kids playing', 0.0899), ('A photo of aeroplane', 0.0159), ('A photo of traffic', 0.0078), ('A photo of elon musk wearing a face mask', 0.0056)]\n",
      "--------------------------------\n",
      "Input: images/elon_masked.jpg\n",
      "Output:\n",
      "[('A photo of elon musk wearing a face mask', 1.0), ('A photo of kids playing', 0.0), ('A photo of dancing', 0.0), ('A photo of aeroplane', 0.0), ('A photo of traffic', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/kids_playing.jpg\n",
      "Output:\n",
      "[('A photo of kids playing', 0.9994), ('A photo of dancing', 0.0005), ('A photo of elon musk wearing a face mask', 0.0), ('A photo of aeroplane', 0.0), ('A photo of traffic', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/plane.jpg\n",
      "Output:\n",
      "[('A photo of aeroplane', 0.9987), ('A photo of traffic', 0.0012), ('A photo of dancing', 0.0001), ('A photo of kids playing', 0.0), ('A photo of elon musk wearing a face mask', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/traffic.jpg\n",
      "Output:\n",
      "[('A photo of traffic', 1.0), ('A photo of kids playing', 0.0), ('A photo of dancing', 0.0), ('A photo of elon musk wearing a face mask', 0.0), ('A photo of aeroplane', 0.0)]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for path, image in zip(image_paths, images):\n",
    "    print(f'Input: {path}')\n",
    "    print('Output:')\n",
    "    print(predict(image, list(map(lambda x: f\"A photo of {x}\", classes))))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5b009",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94547a98",
   "metadata": {},
   "source": [
    "### Test Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0cb357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:23.249501Z",
     "iopub.status.busy": "2021-12-02T20:57:23.249179Z",
     "iopub.status.idle": "2021-12-02T20:57:23.254242Z",
     "shell.execute_reply": "2021-12-02T20:57:23.253553Z",
     "shell.execute_reply.started": "2021-12-02T20:57:23.249474Z"
    }
   },
   "outputs": [],
   "source": [
    "from inference import CLIPImageClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f67df9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:24.026450Z",
     "iopub.status.busy": "2021-12-02T20:57:24.025990Z",
     "iopub.status.idle": "2021-12-02T20:57:24.847812Z",
     "shell.execute_reply": "2021-12-02T20:57:24.847126Z",
     "shell.execute_reply.started": "2021-12-02T20:57:24.026423Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = CLIPImageClassifier(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd001a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T20:57:25.985264Z",
     "iopub.status.busy": "2021-12-02T20:57:25.984729Z",
     "iopub.status.idle": "2021-12-02T20:57:29.291732Z",
     "shell.execute_reply": "2021-12-02T20:57:29.290984Z",
     "shell.execute_reply.started": "2021-12-02T20:57:25.985237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: images/dancing.jpg\n",
      "Output:\n",
      "[('A photo of dancing', 0.8808), ('A photo of kids playing', 0.0899), ('A photo of aeroplane', 0.0159), ('A photo of traffic', 0.0078), ('A photo of elon musk wearing a face mask', 0.0056)]\n",
      "--------------------------------\n",
      "Input: images/elon_masked.jpg\n",
      "Output:\n",
      "[('A photo of elon musk wearing a face mask', 1.0), ('A photo of kids playing', 0.0), ('A photo of dancing', 0.0), ('A photo of aeroplane', 0.0), ('A photo of traffic', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/kids_playing.jpg\n",
      "Output:\n",
      "[('A photo of kids playing', 0.9994), ('A photo of dancing', 0.0005), ('A photo of elon musk wearing a face mask', 0.0), ('A photo of aeroplane', 0.0), ('A photo of traffic', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/plane.jpg\n",
      "Output:\n",
      "[('A photo of aeroplane', 0.9987), ('A photo of traffic', 0.0012), ('A photo of dancing', 0.0001), ('A photo of kids playing', 0.0), ('A photo of elon musk wearing a face mask', 0.0)]\n",
      "--------------------------------\n",
      "Input: images/traffic.jpg\n",
      "Output:\n",
      "[('A photo of traffic', 1.0), ('A photo of kids playing', 0.0), ('A photo of dancing', 0.0), ('A photo of elon musk wearing a face mask', 0.0), ('A photo of aeroplane', 0.0)]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for path, image in zip(image_paths, images):\n",
    "    print(f'Input: {path}')\n",
    "    print('Output:')\n",
    "    print(classifier.predict(image, list(map(lambda x: f\"A photo of {x}\", classes))))\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4675e",
   "metadata": {},
   "source": [
    "### 1. Initialize Hub API Project\n",
    "Open a terminal and run the following command,\n",
    "```\n",
    "hub init clip-classifier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82db4ed",
   "metadata": {},
   "source": [
    "### 2. Integration\n",
    "\n",
    "#### i. Copy the `ViT-B-32.pt` model file to `clip-classifier/model/` folder in Hub API project\n",
    "\n",
    "#### ii. Replace the `clip-classifier/src/main.py` code with this,\n",
    "```python\n",
    "import os\n",
    "from hub import hub_handler\n",
    "# Add your own import statements\n",
    "from inference import CLIPImageClassifier\n",
    "from utils import convert_base64_to_image\n",
    "\n",
    "# This environment variable gives you the\n",
    "# path to the directory of your model. You\n",
    "# can use this in your code to load model\n",
    "# and other large files\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\")\n",
    "classifier = CLIPImageClassifier(os.path.join(MODEL_DIR, 'ViT-B-32.pt'))\n",
    "\n",
    "@hub_handler\n",
    "def inference_handler(inputs, _):\n",
    "    '''The main inference function which gets triggered when the API is invoked'''\n",
    "    \n",
    "    image = convert_base64_to_image(inputs['image'], return_type='pillow')\n",
    "    print(image)\n",
    "    print(inputs['classes'])\n",
    "    \n",
    "    output = classifier.predict(image, inputs['classes'])\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "#### iii. Add the libraries in `zero-shot/src/requirements.txt`\n",
    "```\n",
    "torch\n",
    "git+https://github.com/openai/CLIP.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed619ba8",
   "metadata": {},
   "source": [
    "### 3. Build and Deploy\n",
    "\n",
    "Change directory into the `clip-classifier` project folder in the terminal and then run the following commands,\n",
    "```bash\n",
    "hub build\n",
    "hub deploy\n",
    "```\n",
    "\n",
    "#### Tip:\n",
    "You can directly run the `build` and `deploy` commands on `clip-hub-api` folder without the integration step as it contains the final source code for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df8d83",
   "metadata": {},
   "source": [
    "### Test the Deployed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ef8cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T21:01:18.045684Z",
     "iopub.status.busy": "2021-12-02T21:01:18.045358Z",
     "iopub.status.idle": "2021-12-02T21:01:18.400666Z",
     "shell.execute_reply": "2021-12-02T21:01:18.400081Z",
     "shell.execute_reply.started": "2021-12-02T21:01:18.045660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Paste your key and username here\n",
    "API_KEY = \"CJaYCj7gL3azRRolVuEcm8G9Baam9b8L7m9gW0sl\"\n",
    "USERNAME = \"nerdimite\"\n",
    "API_NAME = \"clip-classifier\" # replace with your project name if you named it anything else other than \"clip-classifier\"\n",
    "\n",
    "# The API endpoint for your Hub API project\n",
    "endpoint = f\"https://api.cellstrathub.com/{USERNAME}/{API_NAME}\"\n",
    "\n",
    "headers = {\n",
    "  \"x-api-key\": API_KEY,\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42016dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T21:01:18.855611Z",
     "iopub.status.busy": "2021-12-02T21:01:18.855269Z",
     "iopub.status.idle": "2021-12-02T21:01:18.862291Z",
     "shell.execute_reply": "2021-12-02T21:01:18.861601Z",
     "shell.execute_reply.started": "2021-12-02T21:01:18.855586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load images as base64 encoded strings\n",
    "image_strings = []\n",
    "\n",
    "# Read all the images\n",
    "for img in os.listdir('images'):\n",
    "    img_path = os.path.join('images', img)\n",
    "\n",
    "    if os.path.isfile(img_path):\n",
    "        \n",
    "        # read the image\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img_bytes = f.read()\n",
    "            \n",
    "            # convert to a base64 string\n",
    "            img_str = base64.b64encode(img_bytes).decode('utf-8')\n",
    "            \n",
    "            image_strings.append(img_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f912dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T21:06:29.014781Z",
     "iopub.status.busy": "2021-12-02T21:06:29.014467Z",
     "iopub.status.idle": "2021-12-02T21:06:30.772465Z",
     "shell.execute_reply": "2021-12-02T21:06:30.771743Z",
     "shell.execute_reply.started": "2021-12-02T21:06:29.014757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [['kids playing', 0.999], ['aeroplane', 0.0006], ['dancing', 0.0004], ['elon musk wearing a face mask', 0.0], ['traffic', 0.0]]\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "    'image': image_strings[2],\n",
    "    'classes': classes\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(endpoint, headers=headers, data=json.dumps(payload)).json()\n",
    "\n",
    "if response.get('statusCode') == 200:\n",
    "    # Parse the output\n",
    "    print('Predictions:', response['body']['output'])\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6de65e-6ec0-4319-92dc-e14e8035c0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200,\n",
       " 'headers': {'Content-Type': 'application/json',\n",
       "  'Access-Control-Allow-Origin': '*'},\n",
       " 'body': 'Model Loaded in Memory'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(endpoint, headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91572bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60fbf1aecf0122793952a73a80d27bc8732eff9e143c13520ca117508929b1c7"
  },
  "kernelspec": {
   "display_name": "PyTorch 1.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
